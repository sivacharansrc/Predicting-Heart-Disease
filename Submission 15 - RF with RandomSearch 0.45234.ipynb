{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"cubehelix\") # Other palettes: \"Set2\", \"husl\", \"cubehelix\", \"hls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input file\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "train_values = cwd + \"\\\\Source Data\\\\train_values.csv\"\n",
    "train_values = pd.read_csv(train_values)\n",
    "train_labels = cwd + \"\\\\Source Data\\\\train_labels.csv\"\n",
    "train_labels = pd.read_csv(train_labels)\n",
    "\n",
    "test_values = cwd + \"\\\\Source Data\\\\test_values.csv\"\n",
    "test_values = pd.read_csv(test_values)\n",
    "test_id = test_values.patient_id\n",
    "test = test_values.drop('patient_id', axis=1)\n",
    "\n",
    "train = train_values.join(train_labels.set_index('patient_id'), on='patient_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep\n",
    "\n",
    "y = train['heart_disease_present']\n",
    "x = train.drop(['heart_disease_present', 'patient_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slope_of_peak_exercise_st_segment         int64\n",
       "thal                                     object\n",
       "resting_blood_pressure                    int64\n",
       "chest_pain_type                           int64\n",
       "num_major_vessels                         int64\n",
       "fasting_blood_sugar_gt_120_mg_per_dl      int64\n",
       "resting_ekg_results                       int64\n",
       "serum_cholesterol_mg_per_dl               int64\n",
       "oldpeak_eq_st_depression                float64\n",
       "sex                                       int64\n",
       "age                                       int64\n",
       "max_heart_rate_achieved                   int64\n",
       "exercise_induced_angina                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to include\n",
    "\n",
    "cols_to_include = list(x.columns)\n",
    "\n",
    "cols_to_include = ['int64', 'float64']\n",
    "x = x.select_dtypes(include=cols_to_include)\n",
    "test = test.select_dtypes(include=cols_to_include)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dummies\n",
    "\n",
    "x_rows = x.shape[0]\n",
    "df = x.append(test)\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "x = df.iloc[:x_rows,]\n",
    "test = df.iloc[x_rows:,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search Optimizer for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': ['linear', 'poly'],\n",
       " 'C': [0.5, 1, 10, 20, 50],\n",
       " 'decision_function_shape': ['ovo', 'ovr'],\n",
       " 'gamma': [1, 5, 30, 50, 100]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Random Grid\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "# model = (n_estimators=500, criterion='gini', max_depth=25, min_samples_leaf=5, oob_score=True, n_jobs=-1, random_state=10)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=50, stop=200, num=20)]\n",
    "\n",
    "# Number of features to consider at every split (For max_features we can also consider sqrt or auto)\n",
    "max_features = [int(x) for x in np.linspace(start= np.int(len(x.columns)/10),\n",
    "                                            stop= np.int(len(x.columns)/2), num=6)]\n",
    "# The split criterion\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 15, num = 10)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [4,5,6,7,10,13]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2,3,4,5,6,8]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_features = [3,4,5,6,7,8]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'criterion': criterion,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'brier_score_loss',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'mutual_info_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   50.3s\n"
     ]
    }
   ],
   "source": [
    "# Performing the Random Search\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# Let us use the base model \"model\" that is already created to tune the model against\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "model_random = RandomizedSearchCV(estimator = model, \n",
    "                                  param_distributions = random_grid, \n",
    "                                  n_iter = 100, cv = 3, scoring='neg_log_loss', verbose=100, random_state=42)\n",
    "# Fit the random search model\n",
    "model_random.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x,y, \n",
    "                                                                stratify=y, \n",
    "                                                                random_state=1, \n",
    "                                                                test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix for Training Data - Logistic Regression\n",
      "[[75  0]\n",
      " [ 0 60]]\n",
      "Confustion Matrix for Test Data - Logistic Regression\n",
      "[[16  9]\n",
      " [ 3 17]]\n",
      "Classification Report for Training Data - Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        75\n",
      "           1       1.00      1.00      1.00        60\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       135\n",
      "   macro avg       1.00      1.00      1.00       135\n",
      "weighted avg       1.00      1.00      1.00       135\n",
      "\n",
      "Classification Report for Test Data - Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.73        25\n",
      "           1       0.65      0.85      0.74        20\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        45\n",
      "   macro avg       0.75      0.74      0.73        45\n",
      "weighted avg       0.76      0.73      0.73        45\n",
      "\n",
      "Log loss for Training data:  0.10221442676494231\n",
      "Log loss for Validation data:  0.511706628080196\n",
      "\n",
      "AUROC Plot from Logistic Regression: 0.8360\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the Random Forest Classifier using the best estimator\n",
    "\n",
    "model = model_random.best_estimator_\n",
    "model.fit(x_train, y_train)\n",
    "train_predictions = model.predict(x_train)\n",
    "predictions = model.predict(x_validation)\n",
    "\n",
    "# Accuracy Metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Confusion Matrix Comparison\n",
    "print(\"Confustion Matrix for Training Data - Logistic Regression\")\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(\"Confustion Matrix for Test Data - Logistic Regression\")\n",
    "print(confusion_matrix(y_validation, predictions))\n",
    "\n",
    "# Classification Report Comparison\n",
    "print(\"Classification Report for Training Data - Logistic Regression\")\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Classification Report for Test Data - Logistic Regression\")\n",
    "print(classification_report(y_validation, predictions))\n",
    "\n",
    "proba_train_predictions = model.predict_proba(x_train)[:,1]\n",
    "proba_predictions = model.predict_proba(x_validation)[:,1]\n",
    "roc_auc_score(y_validation, proba_predictions, average='weighted')\n",
    "\n",
    "# Print the log loss metric\n",
    "\n",
    "print(\"Log loss for Training data: \", log_loss(y_train, proba_train_predictions))\n",
    "print(\"Log loss for Validation data: \", log_loss(y_validation, proba_predictions))\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_validation, proba_predictions)\n",
    "\n",
    "# Calculating the AUC Score\n",
    "auc = np.trapz(tpr,fpr)\n",
    "pltTitle = print(\"\\nAUROC Plot from Logistic Regression:\", \"%.4f\" %auc)\n",
    "\n",
    "# Plotting the ROC Curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.title(pltTitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating output using Logistic Regression without normalizing the data\n",
    "\n",
    "proba_test_predictions = model.predict_proba(test)[:,1]\n",
    "\n",
    "output = pd.DataFrame({'patient_id': test_id, 'heart_disease_present':proba_test_predictions})\n",
    "output.to_csv(cwd + \"\\\\Output Data\\\\Submission 15 - RF with RandomSearch logloss 0.5117.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Submission 13 - Logistic Regression LB 0.397.ipynb.\n",
      "The file will have its original line endings in your working directory.\n",
      "warning: LF will be replaced by CRLF in .ipynb_checkpoints/Submission 15 - RF with RandomSearch 0.45234-checkpoint.ipynb.\n",
      "The file will have its original line endings in your working directory.\n",
      "warning: LF will be replaced by CRLF in Submission 15 - RF with RandomSearch 0.45234.ipynb.\n",
      "The file will have its original line endings in your working directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 643a86d] Submission 15 - RF with RandomSearch 0.45234\n",
      " 4 files changed, 2408 insertions(+), 7 deletions(-)\n",
      " create mode 100644 .ipynb_checkpoints/Submission 15 - RF with RandomSearch 0.45234-checkpoint.ipynb\n",
      " create mode 100644 Output Data/Submission 15 - RF with RandomSearch logloss 0.5117.csv\n",
      " create mode 100644 Submission 15 - RF with RandomSearch 0.45234.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/sivacharansrc/Predicting-Heart-Disease.git\n",
      "   8cf5979..643a86d  master -> master\n"
     ]
    }
   ],
   "source": [
    "# # Pushing the repo to git\n",
    "\n",
    "# ! cd \"C:\\\\Users\\\\sivac\\\\Documents\\\\Python Projects\\\\Driven Data\\\\Predicting Heart Disease\"\n",
    "\n",
    "# ! git add .\n",
    "# ! git commit -am \"Submission 15 - RF with RandomSearch 0.45234\"\n",
    "# ! git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
